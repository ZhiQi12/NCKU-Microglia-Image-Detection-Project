{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4NY8Xk7gSEC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import shutil\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "dea2406b-b135-458f-8ac8-f184190c4f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-339-g150a1a31 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 30.4/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt comet_ml  # install\n",
        "\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCjer4M83zGT",
        "outputId": "687c7bde-15f6-4e58-eed7-b00c431c7baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "train: /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/train\n",
            "val: /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/val\n",
            "\n",
            "nc: 1  # number of classes\n",
            "names: ['microglia']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the paths to your train and val directories\n",
        "train_dir = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/train'\n",
        "val_dir = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/val'\n",
        "\n",
        "class_names_path = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_4/classes.txt'\n",
        "\n",
        "# Read class names from a file\n",
        "with open(class_names_path, 'r') as f:\n",
        "    class_names = f.read().strip().split()\n",
        "\n",
        "# Create the data.yaml content\n",
        "data_yaml_content = f\"\"\"\n",
        "train: {train_dir}\n",
        "val: {val_dir}\n",
        "\n",
        "nc: {len(class_names)}  # number of classes\n",
        "names: {class_names}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Write the data.yaml content to a file\n",
        "with open('data.yaml', 'w') as f:\n",
        "    f.write(data_yaml_content)\n",
        "\n",
        "# Verify the content\n",
        "print(data_yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "91459c33-3aa0-4587-b378-95d2d3c566cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-11 08:29:38.248122: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-11 08:29:38.248180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-11 08:29:38.339671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-321-g3742ab49 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "COMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Using '/content/yolov5/.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 326MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/labels/train.cache... 128 images, 0 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|██████████| 128/128 [01:08<00:00,  1.86it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/labels/val.cache... 64 images, 0 backgrounds, 0 corrupt: 100%|██████████| 64/64 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|██████████| 64/64 [00:55<00:00,  1.15it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.76 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/49      3.66G     0.1273    0.09739          0        254        640: 100%|██████████| 8/8 [00:07<00:00,  1.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:05<00:05,  5.42s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]\n",
            "                   all         64        980     0.0023     0.0296    0.00133   0.000322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/49      4.48G     0.1231      0.108          0        323        640: 100%|██████████| 8/8 [00:02<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:02<00:02,  2.43s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.45s/it]\n",
            "                   all         64        980    0.00425     0.0806    0.00304   0.000738\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/49      4.48G     0.1192     0.1119          0        361        640: 100%|██████████| 8/8 [00:02<00:00,  3.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [00:02<00:02,  2.48s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.49s/it]\n",
            "                   all         64        980     0.0104      0.134    0.00872    0.00191\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/49      4.48G     0.1146      0.102          0        343        640: 100%|██████████| 8/8 [00:03<00:00,  2.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n",
            "                   all         64        980     0.0859        0.1     0.0392    0.00795\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/49      4.48G     0.1088    0.09312          0        253        640: 100%|██████████| 8/8 [00:02<00:00,  3.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.08s/it]\n",
            "                   all         64        980      0.109      0.244     0.0654     0.0144\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/49      4.48G     0.1051    0.08634          0        342        640: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]\n",
            "                   all         64        980      0.159        0.3      0.114      0.024\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/49      4.48G    0.09658    0.08257          0        236        640: 100%|██████████| 8/8 [00:02<00:00,  2.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\n",
            "                   all         64        980       0.12      0.282     0.0934     0.0234\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/49      4.48G    0.09375    0.09189          0        388        640: 100%|██████████| 8/8 [00:02<00:00,  3.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]\n",
            "                   all         64        980      0.107       0.35     0.0956     0.0235\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/49      4.48G    0.08952    0.08793          0        326        640: 100%|██████████| 8/8 [00:02<00:00,  3.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]\n",
            "                   all         64        980       0.21      0.377      0.187     0.0525\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/49      4.48G    0.08667    0.07931          0        312        640: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n",
            "                   all         64        980      0.204      0.466      0.177     0.0498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/49      4.48G    0.08416    0.09566          0        335        640: 100%|██████████| 8/8 [00:02<00:00,  2.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n",
            "                   all         64        980      0.108      0.503     0.0998     0.0318\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/49      4.48G    0.09142    0.09111          0        420        640: 100%|██████████| 8/8 [00:02<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n",
            "                   all         64        980      0.246      0.402      0.206     0.0628\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/49      4.48G    0.09065      0.078          0        387        640: 100%|██████████| 8/8 [00:02<00:00,  3.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.30it/s]\n",
            "                   all         64        980      0.176      0.422      0.152     0.0387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/49      4.48G    0.08409    0.08544          0        334        640: 100%|██████████| 8/8 [00:02<00:00,  3.94it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.14it/s]\n",
            "                   all         64        980      0.257      0.516      0.258     0.0808\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/49      4.48G    0.08114    0.08286          0        315        640: 100%|██████████| 8/8 [00:02<00:00,  3.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n",
            "                   all         64        980      0.125      0.459      0.104     0.0286\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/49      4.48G    0.07587    0.08568          0        445        640: 100%|██████████| 8/8 [00:02<00:00,  2.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
            "                   all         64        980      0.329      0.377       0.32      0.101\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/49      4.48G    0.07101     0.0902          0        328        640: 100%|██████████| 8/8 [00:02<00:00,  2.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n",
            "                   all         64        980      0.221      0.435       0.23     0.0744\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/49      4.48G    0.07707    0.08857          0        447        640: 100%|██████████| 8/8 [00:02<00:00,  3.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n",
            "                   all         64        980      0.421      0.404      0.395      0.143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/49      4.48G     0.0696    0.09228          0        307        640: 100%|██████████| 8/8 [00:02<00:00,  3.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.44it/s]\n",
            "                   all         64        980      0.233      0.449      0.239     0.0892\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/49      4.48G    0.06929    0.07972          0        413        640: 100%|██████████| 8/8 [00:02<00:00,  3.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
            "                   all         64        980       0.26       0.49      0.253     0.0859\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/49      4.48G    0.06766     0.0818          0        321        640: 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n",
            "                   all         64        980      0.241      0.508      0.221     0.0758\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/49      4.48G    0.06478    0.08888          0        342        640: 100%|██████████| 8/8 [00:02<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n",
            "                   all         64        980      0.288      0.523      0.298     0.0957\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/49      4.48G    0.06422    0.08996          0        318        640: 100%|██████████| 8/8 [00:02<00:00,  3.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.26it/s]\n",
            "                   all         64        980      0.365      0.518      0.399      0.151\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/49      4.48G    0.06076    0.08411          0        345        640: 100%|██████████| 8/8 [00:02<00:00,  3.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.35it/s]\n",
            "                   all         64        980      0.327       0.52      0.344      0.118\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/49      4.48G    0.06016     0.0839          0        264        640: 100%|██████████| 8/8 [00:02<00:00,  3.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]\n",
            "                   all         64        980      0.474      0.547       0.49      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/49      4.48G    0.05983    0.08011          0        267        640: 100%|██████████| 8/8 [00:02<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n",
            "                   all         64        980      0.491      0.579      0.515      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/49      4.48G    0.05568    0.07816          0        285        640: 100%|██████████| 8/8 [00:02<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
            "                   all         64        980      0.512      0.609      0.542      0.228\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/49      4.48G    0.05654    0.09167          0        317        640: 100%|██████████| 8/8 [00:02<00:00,  3.57it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.35it/s]\n",
            "                   all         64        980      0.563      0.614      0.582      0.226\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/49      4.48G    0.05546    0.08635          0        296        640: 100%|██████████| 8/8 [00:02<00:00,  3.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.37it/s]\n",
            "                   all         64        980      0.615      0.643       0.63      0.264\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/49      4.48G    0.05566    0.08079          0        239        640: 100%|██████████| 8/8 [00:02<00:00,  3.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n",
            "                   all         64        980      0.669      0.663      0.668      0.307\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/49      4.48G    0.05402    0.08451          0        268        640: 100%|██████████| 8/8 [00:03<00:00,  2.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
            "                   all         64        980      0.651      0.657      0.666      0.304\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/49      4.48G    0.05256    0.08685          0        365        640: 100%|██████████| 8/8 [00:03<00:00,  2.64it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n",
            "                   all         64        980      0.653      0.653      0.664      0.303\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/49      4.48G     0.0518    0.08378          0        435        640: 100%|██████████| 8/8 [00:03<00:00,  2.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
            "                   all         64        980      0.709      0.668      0.713      0.344\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/49      4.48G    0.05055    0.08594          0        355        640: 100%|██████████| 8/8 [00:02<00:00,  3.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]\n",
            "                   all         64        980      0.659      0.664      0.676      0.321\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/49      4.48G    0.05035    0.08421          0        286        640: 100%|██████████| 8/8 [00:01<00:00,  4.01it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n",
            "                   all         64        980      0.705      0.676      0.711      0.349\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/49      4.48G    0.05214    0.08252          0        296        640: 100%|██████████| 8/8 [00:02<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]\n",
            "                   all         64        980      0.725      0.697      0.737      0.366\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/49      4.48G    0.04772    0.07558          0        193        640: 100%|██████████| 8/8 [00:02<00:00,  2.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n",
            "                   all         64        980      0.737      0.695      0.745      0.346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/49      4.48G    0.04814    0.08047          0        293        640: 100%|██████████| 8/8 [00:02<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n",
            "                   all         64        980      0.744      0.689      0.741      0.343\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/49      4.48G    0.04891    0.09484          0        431        640: 100%|██████████| 8/8 [00:02<00:00,  3.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.42it/s]\n",
            "                   all         64        980      0.712      0.702       0.73      0.342\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/49      4.48G    0.04792     0.0785          0        369        640: 100%|██████████| 8/8 [00:02<00:00,  3.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.37it/s]\n",
            "                   all         64        980       0.73      0.705      0.741      0.357\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/49      4.48G    0.04827    0.08553          0        443        640: 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\n",
            "                   all         64        980      0.716      0.671      0.724      0.367\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/49      4.48G    0.04679    0.08478          0        319        640: 100%|██████████| 8/8 [00:02<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
            "                   all         64        980       0.74      0.706      0.753      0.342\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/49      4.48G    0.04495    0.07946          0        313        640: 100%|██████████| 8/8 [00:02<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n",
            "                   all         64        980      0.724      0.723      0.754       0.36\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/49      4.48G    0.04568    0.08021          0        406        640: 100%|██████████| 8/8 [00:02<00:00,  3.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\n",
            "                   all         64        980      0.725       0.68      0.723      0.357\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/49      4.48G    0.04537    0.08243          0        376        640: 100%|██████████| 8/8 [00:02<00:00,  3.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.40it/s]\n",
            "                   all         64        980      0.694      0.683      0.712      0.367\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/49      4.48G    0.04553    0.08647          0        272        640: 100%|██████████| 8/8 [00:02<00:00,  3.84it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.56it/s]\n",
            "                   all         64        980      0.713       0.71      0.745      0.372\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/49      4.48G     0.0445    0.07567          0        323        640: 100%|██████████| 8/8 [00:02<00:00,  3.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n",
            "                   all         64        980      0.723      0.718      0.755      0.393\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/49      4.48G    0.04414    0.07525          0        294        640: 100%|██████████| 8/8 [00:02<00:00,  2.84it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n",
            "                   all         64        980      0.723      0.718      0.754      0.391\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/49      4.48G    0.04445    0.07815          0        437        640: 100%|██████████| 8/8 [00:02<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n",
            "                   all         64        980      0.717      0.713       0.75      0.383\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/49      4.48G    0.04343     0.0822          0        212        640: 100%|██████████| 8/8 [00:02<00:00,  3.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:00<00:00,  2.39it/s]\n",
            "                   all         64        980      0.676      0.752       0.74      0.381\n",
            "\n",
            "50 epochs completed in 0.070 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]\n",
            "                   all         64        980      0.723      0.718      0.755      0.393\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml OfflineExperiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : [OfflineExperiment will get URL after upload]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [35]                  : (1.639857292175293, 3.950711250305176)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP_0.5 [100]      : (0.0013307458412283367, 0.7548493595291463)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP_0.5:0.95 [100] : (0.0003216828978035603, 0.3927533104780393)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/precision [100]    : (0.0023015873015873015, 0.7442921942190966)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/recall [100]       : (0.02959183673469388, 0.751721108863966)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/box_loss [100]       : (0.0434320904314518, 0.12734971940517426)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/cls_loss             : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/obj_loss [100]       : (0.07525482028722763, 0.11192876100540161)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/box_loss [100]         : (0.04269871860742569, 0.12288722395896912)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/cls_loss               : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/obj_loss [100]         : (0.08014297485351562, 0.14957427978515625)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     x/lr0 [100]                : (0.0004960000000000005, 0.0937)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     x/lr1 [100]                : (0.0004960000000000005, 0.007624)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     x/lr2 [100]                : (0.0004960000000000005, 0.007624)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name                        : exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_batch_metrics     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_confusion_matrix  : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_per_class_metrics : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_max_image_uploads     : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_mode                  : online\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_model_name            : yolov5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hasNestedParams             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     offline_experiment          : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     anchor_t            : 4.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     artifact_alias      : latest\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size          : 16\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bbox_interval       : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box                 : 0.05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bucket              : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg                 : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls                 : 0.006250000000000001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls_pw              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees             : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device              : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     entity              : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     evolve              : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     evolve_population   : data/hyps\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fl_gamma            : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr              : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud              : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze              : [0]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h               : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s               : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v               : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|anchor_t        : 4.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|box             : 0.05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|cls             : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|cls_pw          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|copy_paste      : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|degrees         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|fl_gamma        : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|fliplr          : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|flipud          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_h           : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_s           : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_v           : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|iou_t           : 0.2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|lr0             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|lrf             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|mixup           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|momentum        : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|mosaic          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|obj             : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|obj_pw          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|perspective     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|scale           : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|shear           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|translate       : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_bias_lr  : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_epochs   : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_momentum : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|weight_decay    : 0.0005\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     image_weights       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz               : 640\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou_t               : 0.2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     label_smoothing     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     local_rank          : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0                 : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf                 : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup               : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum            : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     multi_scale         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                : exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ndjson_console      : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ndjson_file         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noautoanchor        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noplots             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nosave              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noval               : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     obj                 : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     obj_pw              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer           : SGD\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience            : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project             : runs/train\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     quad                : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect                : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume_evolve       : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir            : runs/train/exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period         : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale               : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed                : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear               : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     sync_bn             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate           : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     upload_dataset      : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_conf_threshold  : 0.001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_iou_threshold   : 0.6\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr      : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs       : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum     : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay        : 0.0005\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers             : 8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset               : 13 (1.13 MB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     confusion-matrix    : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata        : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 6\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still saving offline stats to messages file before program termination (may take up to 120 seconds)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Starting saving the offline archive\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m To upload this offline experiment, run:\n",
            "    comet upload /content/yolov5/.cometml-runs/be06202f8651463584882d44b54117c7.zip\n"
          ]
        }
      ],
      "source": [
        "# Train YOLOv5s on custom_data yaml for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 50 --data data.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AEzoL9UEope",
        "outputId": "949e9672-f049-48ca-c12d-cafab8558b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/MyDrive/NCKU/Neuroscience_proj/weights\n"
          ]
        }
      ],
      "source": [
        "# import shutil\n",
        "\n",
        "# # After training\n",
        "# best_model_path = '/content/yolov5/runs/train/exp/weights/best.pt'  # Update this path as needed\n",
        "# drive_model_path = '/content/drive/MyDrive/NCKU/Neuroscience_proj/weights'  # Update this path as needed\n",
        "\n",
        "# shutil.copy(best_model_path, drive_model_path)\n",
        "# print(f'Model saved to {drive_model_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzGssLTceO7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb20eb0-1f8d-43e3-a28b-0d91875569e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model loaded from /content/drive/MyDrive/NCKU/Neuroscience_proj/weights/best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 v7.0-339-g150a1a31 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the model\n",
        "drive_model_path = '/content/drive/MyDrive/NCKU/Neuroscience_proj/weights/best.pt'  # Update this path as needed\n",
        "session_model_path = '/content/yolov5/best.pt'  # Update this path as needed\n",
        "\n",
        "shutil.copy(drive_model_path, session_model_path)\n",
        "print(f'Model loaded from {drive_model_path}')\n",
        "\n",
        "# Load the model using YOLOv5\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path=session_model_path)\n",
        "print('Model loaded successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9ZbuQCH7FX",
        "outputId": "8fe94f58-46dc-4ba5-9571-9c6501dc7a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/NCKU/Neuroscience_proj/weights/best.pt'], source=/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test/test, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_csv=False, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-339-g150a1a31 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "WARNING ⚠️ NMS time limit 0.550s exceeded\n",
            "image 1/2 /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test/test/13_s46.png: 640x640 115 microglias, 11.6ms\n",
            "image 2/2 /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test/test/14_s47.png: 640x640 123 microglias, 11.6ms\n",
            "Speed: 0.7ms pre-process, 11.6ms inference, 356.7ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp3\u001b[0m\n",
            "2 labels saved to runs/detect/exp3/labels\n"
          ]
        }
      ],
      "source": [
        "#                                need to update here\n",
        "# !python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test --save-txt --save-conf\n",
        "# !python detect.py --weights best.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test/test --save-txt --save-conf\n",
        "\n",
        "!python detect.py --weights /content/drive/MyDrive/NCKU/Neuroscience_proj/weights/best.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test/test --save-txt --save-conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcS1_PlFHG3W"
      },
      "outputs": [],
      "source": [
        "# Path to ground truth and predictions\n",
        "gt_dir = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/labels/test/test'\n",
        "pred_dir = '/content/yolov5/yolov5/yolov5/runs/detect/exp3/labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGrM_o7-K_nw",
        "outputId": "134acccc-6194-41fd-eba7-40460a2a3ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: microglia\n",
            "Best IoU Threshold: 0.1\n",
            "Best Confidence Threshold: 0.1\n",
            "Best F1 Score: 0.8695652173913043\n",
            "Metrics with Best Thresholds: {'true_positives': 210, 'false_positives': 28, 'false_negatives': 35, 'precision': 0.8823529411764706, 'recall': 0.8571428571428571, 'accuracy': 0.7692307692307693, 'f1_score': 0.8695652173913043, 'total_gt': 245, 'total_pred': 238}\n",
            "Total Labeled microglia: 245\n",
            "Total Predicted microglia: 238\n"
          ]
        }
      ],
      "source": [
        "def load_yolo_annotations(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    annotations = []\n",
        "    for line in lines:\n",
        "        values = list(map(float, line.strip().split()))\n",
        "        annotations.append(values)\n",
        "    return np.array(annotations)\n",
        "\n",
        "def iou(box1, box2):\n",
        "    # Convert center x, y, width, height to x1, y1, x2, y2\n",
        "    box1 = [box1[0] - box1[2] / 2, box1[1] - box1[3] / 2, box1[0] + box1[2] / 2, box1[1] + box1[3] / 2]\n",
        "    box2 = [box2[0] - box2[2] / 2, box2[1] - box2[3] / 2, box2[0] + box2[2] / 2, box2[1] + box2[3] / 2]\n",
        "\n",
        "    # Calculate intersection area\n",
        "    x_left = max(box1[0], box2[0])\n",
        "    y_top = max(box1[1], box2[1])\n",
        "    x_right = min(box1[2], box2[2])\n",
        "    y_bottom = min(box1[3], box2[3])\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "\n",
        "    # Calculate union area\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "    return intersection_area / union_area\n",
        "\n",
        "def calculate_metrics(gt_dir, pred_dir, class_names, iou_threshold=0.5, conf_threshold=0.5):\n",
        "    gt_files = sorted([os.path.join(gt_dir, f) for f in os.listdir(gt_dir) if f.endswith('.txt')])\n",
        "    pred_files = sorted([os.path.join(pred_dir, f) for f in os.listdir(pred_dir) if f.endswith('.txt')])\n",
        "\n",
        "    assert len(gt_files) == len(pred_files), \"Mismatch between ground truth and prediction files\"\n",
        "\n",
        "    class_metrics = {class_name: {'tp': 0, 'fp': 0, 'fn': 0, 'total_gt': 0, 'total_pred': 0} for class_name in class_names}\n",
        "\n",
        "    for gt_file, pred_file in zip(gt_files, pred_files):\n",
        "        gt_annotations = load_yolo_annotations(gt_file)\n",
        "        pred_annotations = load_yolo_annotations(pred_file)\n",
        "\n",
        "        # Filter out predictions with low confidence\n",
        "        pred_annotations = [pred for pred in pred_annotations if pred[5] >= conf_threshold]\n",
        "\n",
        "        for class_id, class_name in enumerate(class_names):\n",
        "            gt_class_annotations = [ann for ann in gt_annotations if int(ann[0]) == class_id]\n",
        "            pred_class_annotations = [ann for ann in pred_annotations if int(ann[0]) == class_id]\n",
        "\n",
        "            class_metrics[class_name]['total_gt'] += len(gt_class_annotations)\n",
        "            class_metrics[class_name]['total_pred'] += len(pred_class_annotations)\n",
        "\n",
        "            matched_gt = []\n",
        "            matched_pred = []\n",
        "\n",
        "            for pred_box in pred_class_annotations:\n",
        "                best_iou = 0\n",
        "                best_gt_idx = -1\n",
        "                for gt_idx, gt_box in enumerate(gt_class_annotations):\n",
        "                    if gt_idx in matched_gt:\n",
        "                        continue\n",
        "                    iou_value = iou(pred_box[1:5], gt_box[1:])  # Only compare box coordinates\n",
        "                    if iou_value > best_iou:\n",
        "                        best_iou = iou_value\n",
        "                        best_gt_idx = gt_idx\n",
        "\n",
        "                if best_iou >= iou_threshold:\n",
        "                    class_metrics[class_name]['tp'] += 1\n",
        "                    matched_gt.append(best_gt_idx)\n",
        "                    matched_pred.append(pred_box)\n",
        "                else:\n",
        "                    class_metrics[class_name]['fp'] += 1\n",
        "\n",
        "            class_metrics[class_name]['fn'] += len(gt_class_annotations) - len(matched_gt)\n",
        "\n",
        "    metrics = {}\n",
        "    for class_name in class_names:\n",
        "        tp = class_metrics[class_name]['tp']\n",
        "        fp = class_metrics[class_name]['fp']\n",
        "        fn = class_metrics[class_name]['fn']\n",
        "\n",
        "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "        accuracy = tp / (tp + fp + fn) if tp + fp + fn > 0 else 0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        metrics[class_name] = {\n",
        "            \"true_positives\": tp,\n",
        "            \"false_positives\": fp,\n",
        "            \"false_negatives\": fn,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"total_gt\": class_metrics[class_name]['total_gt'],\n",
        "            \"total_pred\": class_metrics[class_name]['total_pred']\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def find_best_thresholds(gt_dir, pred_dir, class_names, iou_thresholds, conf_thresholds):\n",
        "    best_f1_scores = {class_name: 0 for class_name in class_names}\n",
        "    best_thresholds = {class_name: {'iou': 0, 'conf': 0} for class_name in class_names}\n",
        "\n",
        "    for iou_threshold in iou_thresholds:\n",
        "        for conf_threshold in conf_thresholds:\n",
        "            metrics = calculate_metrics(gt_dir, pred_dir, class_names, iou_threshold=iou_threshold, conf_threshold=conf_threshold)\n",
        "            for class_name in class_names:\n",
        "                if metrics[class_name][\"f1_score\"] > best_f1_scores[class_name]:\n",
        "                    best_f1_scores[class_name] = metrics[class_name][\"f1_score\"]\n",
        "                    best_thresholds[class_name]['iou'] = iou_threshold\n",
        "                    best_thresholds[class_name]['conf'] = conf_threshold\n",
        "\n",
        "    return best_thresholds, best_f1_scores\n",
        "\n",
        "\n",
        "class_names = ['microglia']\n",
        "\n",
        "# Define a range of thresholds to test\n",
        "iou_thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "conf_thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "\n",
        "best_thresholds, best_f1_scores = find_best_thresholds(gt_dir, pred_dir, class_names, iou_thresholds, conf_thresholds)\n",
        "\n",
        "# Calculate metrics with the best thresholds for each class\n",
        "best_metrics = {}\n",
        "for class_name in class_names:\n",
        "    best_iou = best_thresholds[class_name]['iou']\n",
        "    best_conf = best_thresholds[class_name]['conf']\n",
        "    best_metrics[class_name] = calculate_metrics(gt_dir, pred_dir, class_names, iou_threshold=best_iou, conf_threshold=best_conf)[class_name]\n",
        "\n",
        "for class_name in class_names:\n",
        "    print(f\"Class: {class_name}\")\n",
        "    print(f\"Best IoU Threshold: {best_thresholds[class_name]['iou']}\")\n",
        "    print(f\"Best Confidence Threshold: {best_thresholds[class_name]['conf']}\")\n",
        "    print(f\"Best F1 Score: {best_f1_scores[class_name]}\")\n",
        "    print(f\"Metrics with Best Thresholds: {best_metrics[class_name]}\")\n",
        "    print(f\"Total Labeled {class_name}: {best_metrics[class_name]['total_gt']}\")\n",
        "    print(f\"Total Predicted {class_name}: {best_metrics[class_name]['total_pred']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Q84gkVDInA"
      },
      "outputs": [],
      "source": [
        "def calculate_all_thresholds_metrics(gt_dir, pred_dir, class_names, iou_thresholds, conf_thresholds):\n",
        "    rows = []\n",
        "    for iou_threshold in iou_thresholds:\n",
        "        for conf_threshold in conf_thresholds:\n",
        "            metrics = calculate_metrics(gt_dir, pred_dir, class_names, iou_threshold=iou_threshold, conf_threshold=conf_threshold)\n",
        "            for class_name in class_names:\n",
        "                row = {\n",
        "                    \"class\": class_name,\n",
        "                    \"iou_threshold\": iou_threshold,\n",
        "                    \"conf_threshold\": conf_threshold,\n",
        "                    **metrics[class_name]\n",
        "                }\n",
        "                rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "class_names = ['microglia']\n",
        "\n",
        "# Define a range of thresholds to test\n",
        "iou_thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "conf_thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "\n",
        "metrics_table = calculate_all_thresholds_metrics(gt_dir, pred_dir, class_names, iou_thresholds, conf_thresholds)\n",
        "\n",
        "# Print metrics table for a specific IoU and confidence threshold\n",
        "def print_metrics_for_threshold(iou_threshold, conf_threshold):\n",
        "    filtered_table = metrics_table[(metrics_table['iou_threshold'].round(2) == round(iou_threshold, 2)) & (metrics_table['conf_threshold'].round(2) == round(conf_threshold, 2))]\n",
        "    return(filtered_table.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "k8B_c1qfDZFc",
        "outputId": "80e22309-06f8-49ae-bc22-bea450127f8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         0\n",
              "class            microglia\n",
              "iou_threshold          0.1\n",
              "conf_threshold         0.1\n",
              "true_positives         210\n",
              "false_positives         28\n",
              "false_negatives         35\n",
              "precision         0.882353\n",
              "recall            0.857143\n",
              "accuracy          0.769231\n",
              "f1_score          0.869565\n",
              "total_gt               245\n",
              "total_pred             238"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-125f9537-b70a-465e-a777-85f5940deb4e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <td>microglia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>iou_threshold</th>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conf_threshold</th>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true_positives</th>\n",
              "      <td>210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>false_positives</th>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>false_negatives</th>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.769231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score</th>\n",
              "      <td>0.869565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total_gt</th>\n",
              "      <td>245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total_pred</th>\n",
              "      <td>238</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-125f9537-b70a-465e-a777-85f5940deb4e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-125f9537-b70a-465e-a777-85f5940deb4e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-125f9537-b70a-465e-a777-85f5940deb4e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-58f3ec30-6d86-4188-8c8a-7d6d540f4045\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-58f3ec30-6d86-4188-8c8a-7d6d540f4045')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-58f3ec30-6d86-4188-8c8a-7d6d540f4045 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print_metrics_for_threshold(0\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.8823529411764706,\n          \"microglia\",\n          245\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "print_metrics_for_threshold(0.1, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter the metrics table for a specific class (e.g., 'microglia')\n",
        "class_name = 'microglia'\n",
        "filtered_table = metrics_table[metrics_table['class'] == class_name]\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot precision\n",
        "ax.plot(filtered_table['iou_threshold'], filtered_table['precision'], label='Precision', linestyle='-', marker='o')\n",
        "\n",
        "# Plot recall\n",
        "ax.plot(filtered_table['iou_threshold'], filtered_table['recall'], label='Recall', linestyle='--', marker='x')\n",
        "\n",
        "# Plot accuracy\n",
        "ax.plot(filtered_table['iou_threshold'], filtered_table['accuracy'], label='Accuracy', linestyle='-.', marker='s')\n",
        "\n",
        "# Plot F1 score\n",
        "ax.plot(filtered_table['iou_threshold'], filtered_table['f1_score'], label='F1 Score', linestyle=':', marker='d')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('IoU Threshold')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Metrics Across IoU Thresholds')\n",
        "ax.legend()\n",
        "\n",
        "# Show grid\n",
        "ax.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "sG9CG5HSXbzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3DJaewR7M60"
      },
      "outputs": [],
      "source": [
        "# #                                need to update here\n",
        "# !python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test --save-txt --save-conf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_bounding_boxes(layer, annotations, image_shape, color=(255, 255, 255), thickness=2):\n",
        "    # Convert the layer to a 3-channel image if it is a single-channel layer\n",
        "    if len(layer.shape) == 2:\n",
        "        layer = cv2.cvtColor(layer, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    for annotation in annotations:\n",
        "        try:\n",
        "            class_id, x_center, y_center, width, height = annotation\n",
        "        except:\n",
        "            class_id, x_center, y_center, width, height, confidence = annotation\n",
        "\n",
        "        x_center, y_center, width, height = x_center * image_shape[1], y_center * image_shape[0], width * image_shape[1], height * image_shape[0]\n",
        "        x1, y1, x2, y2 = int(x_center - width / 2), int(y_center - height / 2), int(x_center + width / 2), int(y_center + height / 2)\n",
        "\n",
        "        # Draw the rectangle on the layer\n",
        "        cv2.rectangle(layer, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "    return layer\n",
        "\n",
        "def apply_blackout(layer, annotations, image_shape):\n",
        "    mask = np.zeros((layer.shape[0], layer.shape[1]), dtype=np.uint8)\n",
        "    for annotation in annotations:\n",
        "        if len(annotation) == 5:\n",
        "            class_id, x_center, y_center, width, height = annotation\n",
        "        else:\n",
        "            class_id, x_center, y_center, width, height, confidence = annotation\n",
        "        x_center, y_center, width, height = x_center * image_shape[1], y_center * image_shape[0], width * image_shape[1], height * image_shape[0]\n",
        "        x1, y1, x2, y2 = int(x_center - width / 2), int(y_center - height / 2), int(x_center + width / 2), int(y_center + height / 2)\n",
        "        cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)\n",
        "    mask = cv2.resize(mask, (layer.shape[1], layer.shape[0]))\n",
        "    if layer.shape != mask.shape:\n",
        "        raise ValueError(\"Layer and mask dimensions do not match\")\n",
        "    blacked_out_layer = cv2.bitwise_and(layer, layer, mask=mask)\n",
        "    return blacked_out_layer\n",
        "\n",
        "def combine_layers(red_layer, green_layer, blue_layer):\n",
        "    if red_layer.shape != green_layer.shape or red_layer.shape != blue_layer.shape:\n",
        "        raise ValueError(\"All layers must have the same shape\")\n",
        "    combined_image = cv2.merge([blue_layer, green_layer, red_layer])\n",
        "    return combined_image"
      ],
      "metadata": {
        "id": "8M2hqConh6Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test/test'\n",
        "annotation_folder = '/content/yolov5/yolov5/yolov5/runs/detect/exp3/labels'\n",
        "gt_folder = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/labels/test/test'\n",
        "\n",
        "image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "    annotation_path = os.path.join(annotation_folder, os.path.splitext(image_file)[0] + '.txt')\n",
        "    gt_path = os.path.join(gt_folder, os.path.splitext(image_file)[0] + '.txt')\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Could not load image {image_file}\")\n",
        "        continue\n",
        "\n",
        "    blue_channel, green_channel, red_channel = cv2.split(image)\n",
        "\n",
        "    if os.path.exists(annotation_path):\n",
        "        annotations = load_yolo_annotations(annotation_path)\n",
        "    else:\n",
        "        print('Unable to get predicted annotations')\n",
        "        annotations = []\n",
        "\n",
        "    if os.path.exists(gt_path):\n",
        "        gt_annotations = load_yolo_annotations(gt_path)\n",
        "    else:\n",
        "        print('Unable to get GT annotations')\n",
        "        gt_annotations = []\n",
        "\n",
        "    # print(f\"Predicted annotations for {image_file}: {annotations}\")\n",
        "    # print(f\"GT annotations for {image_file}: {gt_annotations}\")\n",
        "\n",
        "    original_with_boxes = draw_bounding_boxes(image.copy(), annotations, image.shape, color=(0, 255, 0))\n",
        "    original_with_gt_boxes = draw_bounding_boxes(image.copy(), gt_annotations, image.shape, color=(0, 255, 0))\n",
        "\n",
        "    blue_with_boxes = draw_bounding_boxes(blue_channel.copy(), annotations, image.shape, color=(0, 0, 255))\n",
        "    green_with_boxes = draw_bounding_boxes(green_channel.copy(), annotations, image.shape, color=(0, 255, 0))\n",
        "    red_with_boxes = draw_bounding_boxes(red_channel.copy(), annotations, image.shape, color=(255, 0, 0))\n",
        "    blacked_out_green = apply_blackout(green_channel.copy(), annotations, image.shape)\n",
        "\n",
        "    combined_image = combine_layers(red_channel, blacked_out_green, blue_channel)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(12, 8))\n",
        "\n",
        "    axes[0,0].imshow(cv2.cvtColor(original_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "    axes[0,0].set_title('Original Image with Predicted Boxes')\n",
        "    axes[0,0].axis('off')\n",
        "\n",
        "    axes[1,0].imshow(cv2.cvtColor(original_with_gt_boxes, cv2.COLOR_BGR2RGB))\n",
        "    axes[1,0].set_title('Original Image with Ground Truth Boxes')\n",
        "    axes[1,0].axis('off')\n",
        "\n",
        "    axes[0,1].imshow(blue_with_boxes, cmap='gray')\n",
        "    axes[0,1].set_title('Blue Channel with Boxes')\n",
        "    axes[0,1].axis('off')\n",
        "\n",
        "    axes[1,1].imshow(red_with_boxes, cmap='gray')\n",
        "    axes[1,1].set_title('Red Channel with Boxes')\n",
        "    axes[1,1].axis('off')\n",
        "\n",
        "    axes[0,2].imshow(green_with_boxes, cmap='gray')\n",
        "    axes[0,2].set_title('Green Channel with Boxes')\n",
        "    axes[0,2].axis('off')\n",
        "\n",
        "    axes[1,2].imshow(blacked_out_green, cmap='gray')\n",
        "    axes[1,2].set_title('Blacked Out Green Channel')\n",
        "    axes[1,2].axis('off')\n",
        "\n",
        "    axes[0,3].imshow(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
        "    axes[0,3].set_title('Combined Image with Blacked Out Green Channel')\n",
        "    axes[0,3].axis('off')\n",
        "\n",
        "    plt.suptitle(image_file)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dolXZ9NphY9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test'\n",
        "annotation_folder = '/content/yolov5/runs/detect/exp2/labels'\n",
        "gt_folder = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/labels/test'\n",
        "\n",
        "image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "    annotation_path = os.path.join(annotation_folder, os.path.splitext(image_file)[0] + '.txt')\n",
        "    gt_path = os.path.join(gt_folder, os.path.splitext(image_file)[0] + '.txt')\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Could not load image {image_file}\")\n",
        "        continue\n",
        "\n",
        "    blue_channel, green_channel, red_channel = cv2.split(image)\n",
        "\n",
        "    if os.path.exists(annotation_path):\n",
        "        annotations = load_yolo_annotations(annotation_path)\n",
        "    else:\n",
        "        print('Unable to get predicted annotations')\n",
        "        annotations = []\n",
        "\n",
        "    if os.path.exists(gt_path):\n",
        "        gt_annotations = load_yolo_annotations(gt_path)\n",
        "    else:\n",
        "        print('Unable to get GT annotations')\n",
        "        gt_annotations = []\n",
        "\n",
        "    # original_with_boxes = draw_bounding_boxes(image.copy(), annotations, image.shape, color=(0, 255, 0))\n",
        "    original_with_gt_boxes = draw_bounding_boxes(image.copy(), gt_annotations, image.shape, color=(0, 255, 0))\n",
        "\n",
        "    # blue_with_boxes = draw_bounding_boxes(blue_channel.copy(), annotations, image.shape, color=(0, 0, 255))\n",
        "    # green_with_boxes = draw_bounding_boxes(green_channel.copy(), annotations, image.shape, color=(0, 255, 0))\n",
        "    # red_with_boxes = draw_bounding_boxes(red_channel.copy(), annotations, image.shape, color=(255, 0, 0))\n",
        "    blacked_out_green = apply_blackout(green_channel.copy(), annotations, image.shape)\n",
        "\n",
        "    combined_image = combine_layers(red_channel, blacked_out_green, blue_channel)\n",
        "    combined_image_with_gt_boxes = draw_bounding_boxes(combined_image.copy(), annotations, image.shape, color=(0, 255, 0))\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
        "\n",
        "    # axes[0,0].imshow(cv2.cvtColor(original_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "    # axes[0,0].set_title('Original Image with Predicted Boxes')\n",
        "    # axes[0,0].axis('off')\n",
        "\n",
        "    # axes[1,0].imshow(cv2.cvtColor(original_with_gt_boxes, cv2.COLOR_BGR2RGB))\n",
        "    # axes[1,0].set_title('Original Image with Ground Truth Boxes')\n",
        "    # axes[1,0].axis('off')\n",
        "\n",
        "    # axes[0,1].imshow(blue_with_boxes, cmap='gray')\n",
        "    # axes[0,1].set_title('Blue Channel with Boxes')\n",
        "    # axes[0,1].axis('off')\n",
        "\n",
        "    # axes[1,1].imshow(red_with_boxes, cmap='gray')\n",
        "    # axes[1,1].set_title('Red Channel with Boxes')\n",
        "    # axes[1,1].axis('off')\n",
        "\n",
        "    # axes[0,2].imshow(green_with_boxes, cmap='gray')\n",
        "    # axes[0,2].set_title('Green Channel with Boxes')\n",
        "    # axes[0,2].axis('off')\n",
        "\n",
        "    # axes[1,2].imshow(blacked_out_green, cmap='gray')\n",
        "    # axes[1,2].set_title('Blacked Out Green Channel')\n",
        "    # axes[1,2].axis('off')\n",
        "\n",
        "    # axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    # axes[0].set_title('Original Image')\n",
        "    # axes[0].axis('off')\n",
        "\n",
        "    # axes[1].imshow(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
        "    # axes[1].set_title('Combined Image with Denoised Green Channel')\n",
        "    # axes[1].axis('off')\n",
        "\n",
        "    axes[0].imshow(cv2.cvtColor(original_with_gt_boxes, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title('Original Image with Ground Truth Boxes')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(cv2.cvtColor(combined_image_with_gt_boxes, cv2.COLOR_BGR2RGB))\n",
        "    axes[1].set_title('Combined Image with Denoised Green Channel')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.suptitle(image_file)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n"
      ],
      "metadata": {
        "id": "FES0zKQ9kkMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/images/test'\n",
        "annotation_folder = '/content/yolov5/runs/detect/exp/labels'\n",
        "gt_folder = '/content/drive/MyDrive/NCKU/Neuroscience_proj/train_data_5/labels/test'\n",
        "\n",
        "image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "    annotation_path = os.path.join(annotation_folder, os.path.splitext(image_file)[0] + '.txt')\n",
        "    gt_path = os.path.join(gt_folder, os.path.splitext(image_file)[0] + '.txt')\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Could not load image {image_file}\")\n",
        "        continue\n",
        "\n",
        "    blue_channel, green_channel, red_channel = cv2.split(image)\n",
        "\n",
        "    if os.path.exists(annotation_path):\n",
        "        annotations = load_yolo_annotations(annotation_path)\n",
        "    else:\n",
        "        print('Unable to get predicted annotations')\n",
        "        annotations = []\n",
        "\n",
        "    if os.path.exists(gt_path):\n",
        "        gt_annotations = load_yolo_annotations(gt_path)\n",
        "    else:\n",
        "        print('Unable to get GT annotations')\n",
        "        gt_annotations = []\n",
        "\n",
        "    original_with_boxes = draw_bounding_boxes(image.copy(), annotations, image.shape, color=(0, 255, 0))\n",
        "    # original_with_gt_boxes = draw_bounding_boxes(image.copy(), gt_annotations, image.shape, color=(0, 255, 0))\n",
        "\n",
        "    blue_with_boxes = draw_bounding_boxes(blue_channel.copy(), annotations, image.shape, color=(0, 0, 255))\n",
        "    green_with_boxes = draw_bounding_boxes(green_channel.copy(), annotations, image.shape, color=(0, 255, 0))\n",
        "    red_with_boxes = draw_bounding_boxes(red_channel.copy(), annotations, image.shape, color=(255, 0, 0))\n",
        "    # blacked_out_green = apply_blackout(green_channel.copy(), annotations, image.shape)\n",
        "\n",
        "    # combined_image = combine_layers(red_channel, blacked_out_green, blue_channel)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "    axes[0,0].imshow(cv2.cvtColor(original_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "    axes[0,0].set_title('Original Image with Predicted Boxes')\n",
        "    axes[0,0].axis('off')\n",
        "\n",
        "    axes[0,1].imshow(blue_with_boxes, cmap='gray')\n",
        "    axes[0,1].set_title('Blue Channel with Boxes')\n",
        "    axes[0,1].axis('off')\n",
        "\n",
        "    axes[1,0].imshow(red_with_boxes, cmap='gray')\n",
        "    axes[1,0].set_title('Red Channel with Boxes')\n",
        "    axes[1,0].axis('off')\n",
        "\n",
        "    axes[1,1].imshow(green_with_boxes, cmap='gray')\n",
        "    axes[1,1].set_title('Green Channel with Boxes')\n",
        "    axes[1,1].axis('off')\n",
        "\n",
        "    # axes[1,2].imshow(blacked_out_green, cmap='gray')\n",
        "    # axes[1,2].set_title('Blacked Out Green Channel')\n",
        "    # axes[1,2].axis('off')\n",
        "\n",
        "    # axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    # axes[0].set_title('Original Image')\n",
        "    # axes[0].axis('off')\n",
        "\n",
        "    # axes[1].imshow(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
        "    # axes[1].set_title('Combined Image with Denoised Green Channel')\n",
        "    # axes[1].axis('off')\n",
        "\n",
        "    plt.suptitle(image_file)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "WmnOKFoKmWjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3415e496-c6ec-4b9c-e8b0-3b3add71d1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unable to get GT annotations\n",
            "Unable to get GT annotations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close(\"all\")"
      ],
      "metadata": {
        "id": "kwpggaAwPsFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}